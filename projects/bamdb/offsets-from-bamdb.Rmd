---
title: "Calculating offsets from BAM database"
author: "Peter Solymos (solymos@ualberta.ca)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    toc: no
---

# Preliminaries

Load (install) some libraries:

```{r}
if (!require(maptools))
    install.packages("maptools")
if (!require(RODBC))
    install.packages("RODBC")
if (!require(mefa4)) {
    if (!require(devtools))
        install.packages("devtools")
    devtools::install_github("psolymos/mefa4")
}
if (!require(QPAD)) {
    if (!require(devtools))
        install.packages("devtools")
    devtools::install_github("psolymos/QPAD")
}
library(RODBC) # MS Access link
library(QPAD) # latest estimates
library(maptools) # for sunrise calculations
library(mefa4) # data manipulation
```

Set the root folder, all paths are going to be relative to this.
Ideally, you only have to change this line and everything will work
on another local machine:

```{r}
ROOT <- "e:/peter/bam/BAMDB_2017_05"
```

# Fetching the data

This is the folder where a Access database can be found.
Now let us establish a connection called `con`
and fetch the data tables that we need, then close the connection:

```{r}
con <- odbcConnectAccess2007(file.path(ROOT, "May13_2017UPdate.accdb"))
ss <- sqlFetch(con, "CovariatesforoffsetMay2017") # SS table
str(ss)
ss$ID <- ss$OBJECTID <- NULL
pk <- sqlFetch(con, "BC_WSI_KMART_PKEY") # PKEY table
str(pk)
pj <- sqlFetch(con, "National_Proj_Summary_V4_2015") # project summary table
pj$SSMA_TimeStamp <- NULL
str(pj)
close(con)
```

# Organizing the data

Join the `ss` and `pk` tables by the `SS` field:

```{r}
dat <- data.frame(pk, ss[match(pk$SS, ss$SS), ])
dat$SS.1 <- NULL
```

## Maximum distance and duration

This is needed because we want to standardize for unequal sampling effort
based on time and area where the counts were coming from.

We find maximum duration (`MAXDUR`) and maximum distance (`MAXDIS`)
from the project summary table based on a join on the `method` field:

```{r}
dat$MAXDUR <- pj$MaxDuration[match(dat$method, pj$Method)]
table(dat$method, dat$MAXDUR, useNA="always")
dat$MAXDIS <- droplevels(pj$Maxdist[match(dat$method, pj$Method)])
levels(dat$MAXDIS) <- toupper(levels(dat$MAXDIS))
levels(dat$MAXDIS)[levels(dat$MAXDIS) == "UNLIMITED"] <- "Inf"
dat$MAXDIS <- as.numeric(as.character(dat$MAXDIS)) / 100
table(dat$method, dat$MAXDIS, useNA="always")
## QCATLAS:118 has distance method D (0-Inf) --> Unlimited
dat$MAXDIS[dat$method == "QCATLAS:118"] <- Inf
```

## Date and time related variables

Date and time, and location is used to find time since local sunrise
(`TSSR`) and ordinal days (`JDAY`). These were used in the removal sampling
to estimate singing rates.

In this database date and time are provided in POSIXct format.
Here is a code to make it from ingredients:

```{r}
## Date/time components
dat$MINUTE[is.na(dat$MINUTE)] <- 0 # exact minute that critical, is not [Yoda]
mm <- ifelse(dat$MINUTE < 10, paste0("0", dat$MINUTE), as.character(dat$MINUTE))
HH <- ifelse(dat$HOUR < 10, paste0("0", dat$HOUR), as.character(dat$HOUR))
dd <- ifelse(dat$DAY < 10, paste0("0", dat$DAY), as.character(dat$DAY))
MM <- ifelse(dat$MONTH < 10, paste0("0", dat$MONTH), as.character(dat$MONTH))
DD <- paste0(dat$YEAR, "-", MM, "-", dd, " ", HH, ":", mm, ":00")
DD <- strptime(DD, "%Y-%m-%e %H:%M:%S")
dat$DATE <- DD
summary(dat$DATE)
class(DD)
class(dat$DATE)
```

Orinal days (days since January 1st):

```{r}
dat$JULIAN <- dat$DATE$yday
dat$JDAY <- DD$yday / 365
summary(dat$JDAY)
## prevent too far extrapolation
dat$JDAY[dat$JDAY < 0.35 | dat$JDAY > 0.55] <- NA
hist(dat$JDAY)
```
Time since local sunrise: this can be tricky due to time zones.
The sunrise time is calculated in machine time (time zone, daylight savings),
and values need to be offset accordingly to reflect local time:

```{r eval=FALSE}
## TSSR = time since sunrise
Coor <- as.matrix(cbind(as.numeric(dat$POINT_X), as.numeric(dat$POINT_Y)))
JL <- as.POSIXct(DD)
subset <- rowSums(is.na(Coor))==0 & !is.na(JL)
sr <- sunriset(Coor[subset,], JL[subset], direction="sunrise", POSIXct.out=FALSE) * 24
dat$srise <- NA
dat$srise[subset] <- sr
dat$start_time <- dat$HOUR + dat$MINUTE/60

TZ <- SS$TZONE[match(dat$SS, rownames(SS))]
lttz <- read.csv("~/repos/bamanalytics/lookup/tzone.csv")
lttz <- nonDuplicated(lttz, Timezone, TRUE)

dat$MDT_offset <- lttz$MDT_offset[match(TZ, rownames(lttz))]
table(TZ, dat$MDT_offset)
dat$TSSR <- (dat$start_time - dat$srise + dat$MDT_offset) / 24
dat$TSSR_orig <- dat$TSSR # keep a full copy
dat$TSSR[dat$start_time > 12] <- NA ## after noon
summary(dat$TSSR)
summary(dat$start_time)

```

## Tree and land cover

Tree cover (`TREE`) and land cover categories were used in distance sampling
to estimate effective detection radii:

Tree cover has sentinel values (`254`, `255`) that fall outside of the 0--100 percent values
(the values `-9999` are likely points falling outside of the layer extent):
```{r}
dat$TREE <- dat$tree
summary(dat$TREE)
dat$TREE[dat$TREE > 100] <- NA
dat$TREE[dat$TREE < 0] <- NA
dat$TREE <- dat$TREE / 100
summary(dat$TREE)
hist(dat$TREE)
```

Here is how we reclass NALCMS based on a lookup table
(change the path as needed, the project can be cloned downloaded
from [https://github.com/psolymos/bamanalytics](https://github.com/psolymos/bamanalytics)).

(The values `-9999` are likely points falling outside of the layer extent,
`0` indicates no data due to e.g. clouds etc. Tropical cover types not considered.)
```{r}
(ltnalc <- read.csv("~/repos/bamanalytics/lookup/nalcms.csv"))
table(dat$NALCMS05, useNA="always")
dat$NALCMS05[dat$NALCMS05 < 0] <- 0
compare_sets(dat$NALCMS05, ltnalc$Value)
dat$LCC2 <- reclass(dat$NALCMS05, ltnalc[,c("Value", "LCC2")], allow_NA=TRUE)
table(dat$NALCMS05, dat$LCC2, useNA="always")
dat$LCC4 <- reclass(dat$NALCMS05, ltnalc[,c("Value", "LCC4")], allow_NA=TRUE)
table(dat$NALCMS05, dat$LCC4, useNA="always")
```

# Calculating offsets

# Saving the results

```{r eval=FALSE}
## JDAY
## TSSR
## OK TREE
## OK LCC
## OK - MAXDUR
## OK - MAXDIS

ROOT <- "e:/peter/bam/Apr2016"


library(mefa4)
library(QPAD)
source("~/repos/bamanalytics/R/dataprocessing_functions.R")

load(file.path(ROOT, "out", paste0("data_package_2016-04-18.Rdata")))

load_BAM_QPAD(3)
getBAMversion()
sppp <- getBAMspecieslist()

offdat <- data.frame(PKEY[,c("PCODE","PKEY","SS","TSSR","JDAY","MAXDUR","MAXDIS","JULIAN")],
    SS[match(PKEY$SS, rownames(SS)),c("TREE","TREE3","HAB_NALC1","HAB_NALC2", "SPRNG")])
offdat$srise <- PKEY$srise + PKEY$MDT_offset
offdat$DSLS <- (offdat$JULIAN - offdat$SPRNG) / 365
summary(offdat)

offdat$JDAY2 <- offdat$JDAY^2
offdat$TSSR2 <- offdat$TSSR^2
offdat$DSLS2 <- offdat$DSLS^2
offdat$LCC4 <- as.character(offdat$HAB_NALC2)
offdat$LCC4[offdat$LCC4 %in% c("Decid", "Mixed")] <- "DecidMixed"
offdat$LCC4[offdat$LCC4 %in% c("Agr","Barren","Devel","Grass", "Shrub")] <- "Open"
offdat$LCC4 <- factor(offdat$LCC4,
        c("DecidMixed", "Conif", "Open", "Wet"))
offdat$LCC2 <- as.character(offdat$LCC4)
offdat$LCC2[offdat$LCC2 %in% c("DecidMixed", "Conif")] <- "Forest"
offdat$LCC2[offdat$LCC2 %in% c("Open", "Wet")] <- "OpenWet"
offdat$LCC2 <- factor(offdat$LCC2, c("Forest", "OpenWet"))
table(offdat$LCC4, offdat$LCC2)
offdat$MAXDIS <- offdat$MAXDIS / 100

Xp <- cbind("(Intercept)"=1, as.matrix(offdat[,c("TSSR","JDAY","DSLS","TSSR2","JDAY2","DSLS2")]))
Xq <- cbind("(Intercept)"=1, TREE=offdat$TREE,
    LCC2OpenWet=ifelse(offdat$LCC2=="OpenWet", 1, 0),
    LCC4Conif=ifelse(offdat$LCC4=="Conif", 1, 0),
    LCC4Open=ifelse(offdat$LCC4=="Open", 1, 0),
    LCC4Wet=ifelse(offdat$LCC4=="Wet", 1, 0))

OFF <- matrix(NA, nrow(offdat), length(sppp))
rownames(OFF) <- offdat$PKEY
colnames(OFF) <- sppp

#spp <- "OVEN"
for (spp in sppp) {
cat(spp, "\n");flush.console()
p <- rep(NA, nrow(offdat))
A <- q <- p

## constant for NA cases
cf0 <- exp(unlist(coefBAMspecies(spp, 0, 0)))
## best model
mi <- bestmodelBAMspecies(spp, type="BIC")
cfi <- coefBAMspecies(spp, mi$sra, mi$edr)

Xp2 <- Xp[,names(cfi$sra),drop=FALSE]
OKp <- rowSums(is.na(Xp2)) == 0
Xq2 <- Xq[,names(cfi$edr),drop=FALSE]
OKq <- rowSums(is.na(Xq2)) == 0

p[!OKp] <- sra_fun(offdat$MAXDUR[!OKp], cf0[1])
unlim <- ifelse(offdat$MAXDIS[!OKq] == Inf, TRUE, FALSE)
A[!OKq] <- ifelse(unlim, pi * cf0[2]^2, pi * offdat$MAXDIS[!OKq]^2)
q[!OKq] <- ifelse(unlim, 1, edr_fun(offdat$MAXDIS[!OKq], cf0[2]))

phi1 <- exp(drop(Xp2[OKp,,drop=FALSE] %*% cfi$sra))
tau1 <- exp(drop(Xq2[OKq,,drop=FALSE] %*% cfi$edr))
p[OKp] <- sra_fun(offdat$MAXDUR[OKp], phi1)
unlim <- ifelse(offdat$MAXDIS[OKq] == Inf, TRUE, FALSE)
A[OKq] <- ifelse(unlim, pi * tau1^2, pi * offdat$MAXDIS[OKq]^2)
q[OKq] <- ifelse(unlim, 1, edr_fun(offdat$MAXDIS[OKq], tau1))

ii <- which(p == 0)
p[ii] <- sra_fun(offdat$MAXDUR[ii], cf0[1])

OFF[,spp] <- log(p) + log(A) + log(q)

}

## checks
(Ra <- apply(OFF, 2, range))
summary(t(Ra))
which(!is.finite(Ra[1,]))
which(!is.finite(Ra[2,]))

## save offsets table and covariates used
SPP <- sppp
save(OFF, SPP,
    file=file.path(ROOT, "out", "offsets-v3_2016-04-18.Rdata"))
offdat <- offdat[,c("PKEY","TSSR","JDAY","DSLS","TREE","LCC4","MAXDUR","MAXDIS")]
save(offdat,
    file=file.path(ROOT, "out", "offsets-v3data_2016-04-18.Rdata"))
```
